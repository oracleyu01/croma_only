# ëŒ€í™”í˜• ì³‡ë´‡ ì• í”Œë¦¬ì¼€ì´ì…˜ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import streamlit as st
import json
from openai import OpenAI
import re
import urllib.request
import urllib.parse
from bs4 import BeautifulSoup
from datetime import datetime
import time

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë¶€ë™ì‚° ê°€ê²© ë¶„ì„ AI ì—ì´ì „íŠ¸", page_icon="ğŸ ")

# OpenAI API í‚¤ ê°€ì ¸ì˜¤ê¸°
def get_api_key():
    try:
        return st.secrets["OPENAI_API_KEY"]
    except:
        return st.sidebar.text_input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.title("ì„¤ì •")
api_key = get_api_key()

if api_key and api_key.startswith("sk-"):
    st.sidebar.success("âœ… API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤")
else:
    st.sidebar.warning("âš ï¸ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")

st.title("ğŸ  ë¶€ë™ì‚° ê°€ê²© ë¶„ì„ AI ë°ì´í„° ë¶„ì„ê°€")
st.write("ì¤‘ì•™ì¼ë³´ì˜ ìµœì‹  ë¶€ë™ì‚° ë‰´ìŠ¤ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")

# ê¸°ë³¸ ìƒ˜í”Œ ë°ì´í„° (í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œ ë°±ì—…)
DEFAULT_ARTICLES = [
    {
        "title": "ê°•ë‚¨ ì•„íŒŒíŠ¸ê°’ 3ê°œì›” ì—°ì† ìƒìŠ¹ì„¸",
        "content": "ì„œìš¸ ê°•ë‚¨êµ¬ ì•„íŒŒíŠ¸ ë§¤ë§¤ê°€ê²©ì´ 3ê°œì›” ì—°ì† ìƒìŠ¹ì„¸ë¥¼ ì´ì–´ê°”ë‹¤. í•œêµ­ë¶€ë™ì‚°ì›ì— ë”°ë¥´ë©´...",
        "date": "2025-01-15",
        "region": "ê°•ë‚¨",
        "price_trend": "ìƒìŠ¹",
        "url": "#",
        "source": "ìƒ˜í”Œë°ì´í„°"
    }
]

# ê°„ë‹¨í•œ í¬ë¡¤ë§ í•¨ìˆ˜
@st.cache_data(ttl=1800)  # 30ë¶„ ìºì‹œ
def simple_crawl_joongang(keyword="ë¶€ë™ì‚° ê°€ê²©", max_articles=5):
    """ì¤‘ì•™ì¼ë³´ì—ì„œ ê°„ë‹¨í•˜ê²Œ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    try:
        encoded_keyword = urllib.parse.quote(keyword)
        url = f"https://www.joongang.co.kr/search/news?keyword={encoded_keyword}"
        
        # User-Agent ì„¤ì •
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        request = urllib.request.Request(url, headers=headers)
        response = urllib.request.urlopen(request, timeout=10)
        html = response.read().decode('utf-8')
        
        soup = BeautifulSoup(html, 'html.parser')
        
        articles = []
        
        # ê¸°ì‚¬ ì¹´ë“œ ì°¾ê¸°
        article_cards = soup.select('div.card_body')[:max_articles]
        
        for card in article_cards:
            try:
                # ì œëª©ê³¼ ë§í¬
                title_elem = card.select_one('h2.headline a')
                if not title_elem:
                    continue
                    
                title = title_elem.text.strip()
                link = title_elem.get('href', '#')
                
                # ë‚ ì§œ
                date_elem = card.select_one('p.date')
                date = date_elem.text.strip() if date_elem else datetime.now().strftime("%Y-%m-%d")
                
                # ìš”ì•½
                summary_elem = card.select_one('p.description')
                summary = summary_elem.text.strip() if summary_elem else title
                
                # ë¶€ë™ì‚° ê°€ê²© ê´€ë ¨ ê¸°ì‚¬ì¸ì§€ í™•ì¸
                price_keywords = ["ê°€ê²©", "ì‹œì„¸", "ë§¤ë§¤", "ì „ì„¸", "ìƒìŠ¹", "í•˜ë½"]
                if not any(kw in title or kw in summary for kw in price_keywords):
                    continue
                
                # ì§€ì—­ ì¶”ì¶œ
                regions = ["ê°•ë‚¨", "ì„œì´ˆ", "ì†¡íŒŒ", "ë§ˆí¬", "ì„œìš¸", "ê²½ê¸°"]
                article_region = "ê¸°íƒ€"
                for region in regions:
                    if region in title or region in summary:
                        article_region = region
                        break
                
                # ê°€ê²© ë™í–¥
                if any(word in title + summary for word in ["ìƒìŠ¹", "ì˜¬ë¼", "ê¸‰ë“±"]):
                    trend = "ìƒìŠ¹"
                elif any(word in title + summary for word in ["í•˜ë½", "ë‚´ë ¤", "ê¸‰ë½"]):
                    trend = "í•˜ë½"
                else:
                    trend = "ë³´í•©"
                
                articles.append({
                    "title": title,
                    "content": summary,
                    "date": date[:10] if len(date) > 10 else date,
                    "region": article_region,
                    "price_trend": trend,
                    "url": link,
                    "source": "ì¤‘ì•™ì¼ë³´"
                })
                
            except Exception as e:
                continue
        
        return articles if articles else DEFAULT_ARTICLES
        
    except Exception as e:
        st.warning(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return DEFAULT_ARTICLES

# ê²€ìƒ‰ í•¨ìˆ˜
def search_articles(query, articles):
    """ê¸°ì‚¬ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    if not articles:
        return []
    
    query_lower = query.lower()
    results = []
    
    for article in articles:
        score = 0
        
        # ì œëª©, ë‚´ìš©, ì§€ì—­ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in query_lower.split():
            if keyword in article["title"].lower():
                score += 3
            if keyword in article["content"].lower():
                score += 2
            if keyword in article["region"].lower():
                score += 2
        
        if score > 0:
            results.append((score, article))
    
    results.sort(key=lambda x: x[0], reverse=True)
    return [r[1] for r in results[:3]]

# GPT ì‘ë‹µ ìƒì„±
def get_gpt_response(query, articles):
    if not api_key:
        return simple_response(query, articles)
    
    try:
        client = OpenAI(api_key=api_key.strip())
        
        context = "ìµœì‹  ë¶€ë™ì‚° ë‰´ìŠ¤:\n\n"
        for i, article in enumerate(articles[:3]):
            context += f"{i+1}. {article['title']} ({article['date']})\n"
            context += f"   ì§€ì—­: {article['region']} | ë™í–¥: {article['price_trend']}\n"
            context += f"   {article['content'][:200]}...\n\n"
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë¶€ë™ì‚° ì „ë¬¸ê°€ë¡œì„œ ìµœì‹  ë‰´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."},
                {"role": "user", "content": f"{context}\nì§ˆë¬¸: {query}"}
            ],
            max_tokens=500
        )
        
        return response.choices[0].message.content
        
    except:
        return simple_response(query, articles)

# ê°„ë‹¨í•œ ì‘ë‹µ (API í‚¤ ì—†ì„ ë•Œ)
def simple_response(query, articles):
    if not articles:
        return "ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    response = f"'{query}'ì— ëŒ€í•œ ìµœì‹  ë‰´ìŠ¤:\n\n"
    for i, article in enumerate(articles[:3]):
        response += f"**{i+1}. {article['title']}**\n"
        response += f"- ë‚ ì§œ: {article['date']}\n"
        response += f"- ì§€ì—­: {article['region']}\n"
        response += f"- ë™í–¥: {article['price_trend']}\n"
        response += f"- {article['content'][:150]}...\n\n"
    
    return response

# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "articles" not in st.session_state:
    st.session_state.articles = []

# ë°ì´í„° ìˆ˜ì§‘ ì„¹ì…˜
col1, col2 = st.sidebar.columns(2)

with col1:
    if st.button("ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘", type="primary", use_container_width=True):
        with st.spinner("ì¤‘ì•™ì¼ë³´ì—ì„œ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
            articles = simple_crawl_joongang("ë¶€ë™ì‚° ê°€ê²©", 10)
            st.session_state.articles = articles
            st.success(f"{len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ!")

with col2:
    if st.button("ğŸ—‘ï¸ ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì •ë³´
if st.session_state.articles:
    st.sidebar.success(f"ğŸ“Š {len(st.session_state.articles)}ê°œ ê¸°ì‚¬ ë³´ìœ ")
    
    with st.sidebar.expander("ìˆ˜ì§‘ëœ ê¸°ì‚¬"):
        for art in st.session_state.articles[:5]:
            st.write(f"â€¢ {art['title'][:30]}...")
            st.caption(f"{art['date']} | {art['region']}")

# ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
if not st.session_state.articles:
    st.info("ğŸ’¡ ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ 'ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”!")

# ëŒ€í™” í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸í•˜ì„¸ìš” (ì˜ˆ: ê°•ë‚¨ ì•„íŒŒíŠ¸ ê°€ê²©ì€?)"):
    # ì‚¬ìš©ì ë©”ì‹œì§€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)
    
    # ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        if not st.session_state.articles:
            response = "ë¨¼ì € ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”! ì‚¬ì´ë“œë°”ì˜ 'ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”."
            st.write(response)
        else:
            with st.spinner("ë¶„ì„ ì¤‘..."):
                # ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰
                relevant = search_articles(prompt, st.session_state.articles)
                
                # ì‘ë‹µ ìƒì„±
                if relevant:
                    response = get_gpt_response(prompt, relevant)
                else:
                    response = "ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”."
                
                st.write(response)
                
                # ê´€ë ¨ ê¸°ì‚¬ ë§í¬ í‘œì‹œ
                if relevant:
                    with st.expander("ğŸ“ ê´€ë ¨ ê¸°ì‚¬"):
                        for art in relevant[:3]:
                            if art['url'] != '#':
                                st.write(f"- [{art['title']}]({art['url']})")
                            else:
                                st.write(f"- {art['title']}")
    
    st.session_state.messages.append({"role": "assistant", "content": response})

# ì˜ˆì‹œ ì§ˆë¬¸
st.sidebar.header("ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸")
for ex in ["ê°•ë‚¨ ì•„íŒŒíŠ¸ ê°€ê²©", "ì „ì„¸ ì‹œì¥ ë™í–¥", "ë¶€ë™ì‚° ê°€ê²© ì „ë§"]:
    if st.sidebar.button(ex):
        st.rerun()
