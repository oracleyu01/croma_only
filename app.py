# ëŒ€í™”í˜• ì³‡ë´‡ ì• í”Œë¦¬ì¼€ì´ì…˜ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import streamlit as st
import json
from openai import OpenAI
import re
import urllib.request
import urllib.parse
from bs4 import BeautifulSoup
from datetime import datetime
import time
import os

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë¶€ë™ì‚° ê°€ê²© ë¶„ì„ AI ì—ì´ì „íŠ¸", page_icon="ğŸ ")

# OpenAI API í‚¤ ê°€ì ¸ì˜¤ê¸°
def get_api_key():
    """Streamlit secrets ë˜ëŠ” ì‚¬ì´ë“œë°” ì…ë ¥ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°"""
    try:
        return st.secrets["OPENAI_API_KEY"]
    except:
        return st.sidebar.text_input(
            "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”", 
            type="password",
            help="Streamlit Cloudì—ì„œëŠ” Settings > Secretsì—ì„œ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”"
        )

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.title("ì„¤ì •")
api_key = get_api_key()

# API í‚¤ ìƒíƒœ í‘œì‹œ
if api_key and api_key.startswith("sk-"):
    st.sidebar.success("âœ… API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤")
else:
    st.sidebar.warning("âš ï¸ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")

# í˜ì´ì§€ ì œëª©
st.title("ğŸ  ë¶€ë™ì‚° ê°€ê²© ë¶„ì„ AI ë°ì´í„° ë¶„ì„ê°€")
st.write("ì¤‘ì•™ì¼ë³´ì˜ ìµœì‹  ë¶€ë™ì‚° ë‰´ìŠ¤ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")

# í¬ë¡¤ë§ í•¨ìˆ˜ë“¤
@st.cache_data(ttl=3600)  # 1ì‹œê°„ ìºì‹œ
def get_article_details_url(keyword, num_pages=2):
    """ì¤‘ì•™ì¼ë³´ì—ì„œ í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ URL ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    encoded_keyword = urllib.parse.quote(keyword)
    article_urls = []

    progress_bar = st.progress(0)
    status_text = st.empty()

    for page in range(1, num_pages + 1):
        list_url = f"https://www.joongang.co.kr/search/news?keyword={encoded_keyword}&page={page}"
        try:
            status_text.text(f"í˜ì´ì§€ {page}/{num_pages} ê²€ìƒ‰ ì¤‘...")
            request = urllib.request.Request(list_url)
            request.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
            
            response = urllib.request.urlopen(request).read().decode("utf-8")
            soup = BeautifulSoup(response, "html.parser")

            # ìƒì„¸ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
            for headline in soup.select("div.card_body > h2.headline > a"):
                article_url = headline.get("href")
                if article_url:
                    article_urls.append(article_url)

            progress_bar.progress(page / num_pages)
            time.sleep(0.5)  # ì„œë²„ ë¶€ë‹´ ê°ì†Œ

        except Exception as e:
            st.warning(f"í˜ì´ì§€ {page} ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    progress_bar.empty()
    status_text.empty()
    return article_urls[:10]  # ìµœëŒ€ 10ê°œ ê¸°ì‚¬ë§Œ ë°˜í™˜

def is_price_related_article(title, content):
    """ê¸°ì‚¬ê°€ ë¶€ë™ì‚° ê°€ê²©ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    price_keywords = [
        "ê°€ê²©", "ì‹œì„¸", "ë§¤ë§¤", "ì „ì„¸", "ì›”ì„¸", "ìƒìŠ¹", "í•˜ë½", "ê±°ë˜", 
        "ê¸‰ë“±", "ê¸‰ë½", "ë§¤ë§¤ê°€", "ì „ì…‹ê°’", "ì§‘ê°’", "ì•„íŒŒíŠ¸ê°’"
    ]
    
    if any(keyword in title for keyword in price_keywords):
        return True
    
    content_preview = content[:1000] if content else ""
    if any(keyword in content_preview for keyword in price_keywords):
        if re.search(r'\d+\s*ì–µ|\d+\s*ì²œ\s*ë§Œ|\d+\s*ë§Œ', content_preview):
            return True
    
    return False

@st.cache_data(ttl=3600)
def extract_article_content(url):
    """URLì—ì„œ ê¸°ì‚¬ì˜ ìƒì„¸ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        request = urllib.request.Request(url)
        request.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        response = urllib.request.urlopen(request).read().decode('utf-8')
        soup = BeautifulSoup(response, 'html.parser')

        # ê¸°ì‚¬ ì œëª© ì¶”ì¶œ
        title_element = soup.select_one("h1.headline, h2.headline")
        title = title_element.text.strip() if title_element else "ì œëª© ì—†ìŒ"

        # ë‚ ì§œ ì¶”ì¶œ
        date_element = soup.select_one("time")
        date = date_element['datetime'][:10] if date_element and 'datetime' in date_element.attrs else datetime.now().strftime("%Y-%m-%d")

        # ë³¸ë¬¸ ì¶”ì¶œ
        content_elements = soup.select('div.article_body p, article.article p')
        content = "\n".join([p.text.strip() for p in content_elements])
        
        if not content:  # ë‹¤ë¥¸ ì„ íƒì ì‹œë„
            content_elements = soup.select('div.article_body, div#article_body')
            content = content_elements[0].text.strip() if content_elements else ""

        # ë¶€ë™ì‚° ê°€ê²©ê³¼ ê´€ë ¨ëœ ê¸°ì‚¬ì¸ì§€ í™•ì¸
        if not is_price_related_article(title, content):
            return None

        # ì§€ì—­ ì •ë³´ ì¶”ì¶œ
        regions = ["ê°•ë‚¨", "ê°•ë¶", "ì„œì´ˆ", "ì†¡íŒŒ", "ë§ˆí¬", "ìš©ì‚°", "ê°•ë™", "ë…¸ì›", "ë¶„ë‹¹", "ì¼ì‚°", "ì„œìš¸", "ê²½ê¸°"]
        article_region = "ê¸°íƒ€"
        for region in regions:
            if region in title or region in content[:500]:
                article_region = region
                break

        # ê°€ê²© ë™í–¥ ë¶„ì„
        price_trend = "ê¸°íƒ€"
        if re.search(r'(ìƒìŠ¹|ì˜¬ë|ì¦ê°€|ì˜¤ë¦„ì„¸|ê¸‰ë“±)', content[:500]):
            price_trend = "ìƒìŠ¹"
        elif re.search(r'(í•˜ë½|ë–¨ì–´ì¡Œ|ê°ì†Œ|ë‚´ë¦¼ì„¸|ê¸‰ë½)', content[:500]):
            price_trend = "í•˜ë½"
        elif re.search(r'(ìœ ì§€|ë³´í•©|ë³€ë™ì—†|ì•ˆì •)', content[:500]):
            price_trend = "ì•ˆì •"

        return {
            "title": title,
            "content": content[:2000],  # ì²˜ìŒ 2000ìë§Œ ì €ì¥
            "url": url,
            "date": date,
            "region": article_region,
            "price_trend": price_trend,
            "source": "ì¤‘ì•™ì¼ë³´"
        }

    except Exception as e:
        st.warning(f"ê¸°ì‚¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
def collect_real_estate_news(keyword="ë¶€ë™ì‚° ê°€ê²©", num_pages=2):
    """ì‹¤ì‹œê°„ìœ¼ë¡œ ì¤‘ì•™ì¼ë³´ ë¶€ë™ì‚° ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    with st.spinner(f"'{keyword}' ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        # URL ìˆ˜ì§‘
        article_urls = get_article_details_url(keyword, num_pages)
        
        if not article_urls:
            st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ
        articles = []
        progress_bar = st.progress(0)
        
        for i, url in enumerate(article_urls):
            article_data = extract_article_content(url)
            if article_data:
                articles.append(article_data)
            
            progress_bar.progress((i + 1) / len(article_urls))
            time.sleep(0.3)  # ì„œë²„ ë¶€ë‹´ ê°ì†Œ
        
        progress_bar.empty()
        
        return articles

# ê²€ìƒ‰ í•¨ìˆ˜
def search_articles(query, articles):
    """ìˆ˜ì§‘ëœ ê¸°ì‚¬ì—ì„œ ê´€ë ¨ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    if not articles:
        return []
    
    query_lower = query.lower()
    results = []
    
    for article in articles:
        score = 0
        title_lower = article["title"].lower()
        content_lower = article["content"].lower()
        
        # ì¿¼ë¦¬ í‚¤ì›Œë“œ ë§¤ì¹­
        keywords = query_lower.split()
        for keyword in keywords:
            if keyword in title_lower:
                score += 5
            if keyword in content_lower:
                score += 2
            if keyword in article["region"].lower():
                score += 3
        
        if score > 0:
            results.append((score, article))
    
    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    results.sort(key=lambda x: x[0], reverse=True)
    return [r[1] for r in results[:5]]  # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜

# OpenAI ì‘ë‹µ ìƒì„±
def get_gpt_response(query, search_results):
    if not api_key:
        return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    if not search_results:
        return "ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        client = OpenAI(api_key=api_key.strip())
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "ë‹¤ìŒì€ ì¤‘ì•™ì¼ë³´ì˜ ìµœì‹  ë¶€ë™ì‚° ë‰´ìŠ¤ì…ë‹ˆë‹¤:\n\n"
        
        for i, article in enumerate(search_results):
            context += f"[ê¸°ì‚¬ {i+1}]\n"
            context += f"ì œëª©: {article['title']}\n"
            context += f"ë‚ ì§œ: {article['date']}\n"
            context += f"ì§€ì—­: {article['region']}\n"
            context += f"ê°€ê²©ë™í–¥: {article['price_trend']}\n"
            context += f"ë‚´ìš©: {article['content'][:500]}...\n"
            context += f"ì¶œì²˜: {article['url']}\n\n"
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë¶€ë™ì‚° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìµœì‹  ë‰´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."},
                {"role": "user", "content": f"{context}\nì§ˆë¬¸: {query}"}
            ],
            temperature=0.7,
            max_tokens=800
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "articles" not in st.session_state:
    st.session_state.articles = []
if "last_crawl" not in st.session_state:
    st.session_state.last_crawl = None

# ì‚¬ì´ë“œë°” - ë°ì´í„° ìˆ˜ì§‘
st.sidebar.header("ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘")

# ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì •
search_keywords = st.sidebar.text_input(
    "ê²€ìƒ‰ í‚¤ì›Œë“œ", 
    value="ë¶€ë™ì‚° ê°€ê²©",
    help="ì¤‘ì•™ì¼ë³´ì—ì„œ ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”"
)

# í˜ì´ì§€ ìˆ˜ ì„¤ì •
num_pages = st.sidebar.slider("ê²€ìƒ‰ í˜ì´ì§€ ìˆ˜", 1, 5, 2)

# ë°ì´í„° ìˆ˜ì§‘ ë²„íŠ¼
if st.sidebar.button("ğŸ”„ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘", type="primary"):
    articles = collect_real_estate_news(search_keywords, num_pages)
    if articles:
        st.session_state.articles = articles
        st.session_state.last_crawl = datetime.now()
        st.success(f"âœ… {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
    else:
        st.warning("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ìˆ˜ì§‘ëœ ë°ì´í„° ì •ë³´
if st.session_state.articles:
    st.sidebar.success(f"ğŸ“Š ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(st.session_state.articles)}ê°œ")
    if st.session_state.last_crawl:
        st.sidebar.info(f"ë§ˆì§€ë§‰ ìˆ˜ì§‘: {st.session_state.last_crawl.strftime('%Y-%m-%d %H:%M')}")
else:
    st.sidebar.info("ì•„ì§ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n'ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

# ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë¯¸ë¦¬ë³´ê¸°
if st.sidebar.checkbox("ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë³´ê¸°") and st.session_state.articles:
    with st.sidebar.expander("ìµœê·¼ ê¸°ì‚¬ ëª©ë¡"):
        for article in st.session_state.articles[:5]:
            st.write(f"**{article['title'][:30]}...**")
            st.write(f"ğŸ“… {article['date']} | ğŸ“ {article['region']} | ğŸ“ˆ {article['price_trend']}")
            st.write("---")

# ëŒ€í™” í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ê°•ë‚¨ ì•„íŒŒíŠ¸ ê°€ê²©ì€ ì–´ë–»ê²Œ ë˜ê³  ìˆë‚˜ìš”?)"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)
    
    # ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        if not st.session_state.articles:
            response = "ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ 'ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”."
            st.write(response)
        else:
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                # ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰
                relevant_articles = search_articles(prompt, st.session_state.articles)
                
                if api_key:
                    response = get_gpt_response(prompt, relevant_articles)
                else:
                    # API í‚¤ ì—†ì´ ê°„ë‹¨í•œ ì‘ë‹µ
                    response = f"ê´€ë ¨ ê¸°ì‚¬ {len(relevant_articles)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n"
                    for i, article in enumerate(relevant_articles[:3]):
                        response += f"**{i+1}. {article['title']}**\n"
                        response += f"- ë‚ ì§œ: {article['date']}\n"
                        response += f"- ì§€ì—­: {article['region']}\n"
                        response += f"- ë™í–¥: {article['price_trend']}\n"
                        response += f"- ë‚´ìš©: {article['content'][:200]}...\n\n"
                
                st.write(response)
                
                # ì¶œì²˜ í‘œì‹œ
                if relevant_articles:
                    with st.expander("ğŸ“ ì°¸ê³  ê¸°ì‚¬"):
                        for article in relevant_articles[:3]:
                            st.write(f"- [{article['title']}]({article['url']})")
    
    st.session_state.messages.append({"role": "assistant", "content": response})

# ì˜ˆì‹œ ì§ˆë¬¸
st.sidebar.header("ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸")
examples = [
    "ê°•ë‚¨ ì•„íŒŒíŠ¸ ê°€ê²© ë™í–¥ì€?",
    "ìµœê·¼ ì „ì„¸ ì‹œì¥ì€ ì–´ë–¤ê°€ìš”?",
    "ì„œìš¸ ë¶€ë™ì‚° ê°€ê²©ì´ ì˜¤ë¥´ê³  ìˆë‚˜ìš”?",
    "ì†¡íŒŒêµ¬ ì•„íŒŒíŠ¸ ì‹œì„¸ëŠ”?"
]

for example in examples:
    if st.sidebar.button(example, key=f"ex_{example}"):
        st.rerun()

# ëŒ€í™” ì´ˆê¸°í™”
if st.sidebar.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
    st.session_state.messages = []
    st.rerun()
