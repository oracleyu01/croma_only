# ëŒ€í™”í˜• ì³‡ë´‡ ì• í”Œë¦¬ì¼€ì´ì…˜ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import streamlit as st
import json
from openai import OpenAI
import re
import urllib.request
import urllib.parse
from bs4 import BeautifulSoup
from datetime import datetime
import time

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="AI ë‰´ìŠ¤ ë¶„ì„ ì—ì´ì „íŠ¸", page_icon="ğŸ“°")

# OpenAI API í‚¤ ê°€ì ¸ì˜¤ê¸°
def get_api_key():
    try:
        return st.secrets["OPENAI_API_KEY"]
    except:
        return st.sidebar.text_input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.title("âš™ï¸ ì„¤ì •")
api_key = get_api_key()

if api_key and api_key.startswith("sk-"):
    st.sidebar.success("âœ… API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤")
else:
    st.sidebar.warning("âš ï¸ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")

st.title("ğŸ“° AI ë‰´ìŠ¤ ë¶„ì„ ì—ì´ì „íŠ¸")
st.write("ì¤‘ì•™ì¼ë³´ì˜ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê²€ìƒ‰í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.")

# ìµœì‹  ì´ìŠˆ í‚¤ì›Œë“œ (ì£¼ê¸°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•„ìš”)
TRENDING_TOPICS = {
    "ê²½ì œ": ["ê¸ˆë¦¬", "í™˜ìœ¨", "ì£¼ì‹", "ë¶€ë™ì‚°", "ë¬¼ê°€", "ì‹¤ì—…ë¥ "],
    "ì •ì¹˜": ["êµ­íšŒ", "ëŒ€í†µë ¹", "ì„ ê±°", "ì •ë‹¹", "ì™¸êµ", "ì •ì±…"],
    "ì‚¬íšŒ": ["êµìœ¡", "ì˜ë£Œ", "ë³µì§€", "ë²”ì£„", "í™˜ê²½", "ë…¸ë™"],
    "IT/ê³¼í•™": ["AI", "ë°˜ë„ì²´", "ì „ê¸°ì°¨", "ë°”ì´ì˜¤", "ìš°ì£¼", "ë©”íƒ€ë²„ìŠ¤"],
    "ë¬¸í™”": ["KíŒ", "ë“œë¼ë§ˆ", "ì˜í™”", "ìŠ¤í¬ì¸ ", "ê²Œì„", "ê´€ê´‘"],
    "êµ­ì œ": ["ë¯¸êµ­", "ì¤‘êµ­", "ì¼ë³¸", "ëŸ¬ì‹œì•„", "ìœ ëŸ½", "ì¤‘ë™"]
}

# í¬ë¡¤ë§ í•¨ìˆ˜
@st.cache_data(ttl=1800)  # 30ë¶„ ìºì‹œ
def crawl_joongang_news(keyword="ìµœì‹  ë‰´ìŠ¤", max_articles=10):
    """ì¤‘ì•™ì¼ë³´ì—ì„œ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        encoded_keyword = urllib.parse.quote(keyword)
        url = f"https://www.joongang.co.kr/search/news?keyword={encoded_keyword}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        request = urllib.request.Request(url, headers=headers)
        response = urllib.request.urlopen(request, timeout=10)
        html = response.read().decode('utf-8')
        
        soup = BeautifulSoup(html, 'html.parser')
        articles = []
        
        # ê¸°ì‚¬ ì¹´ë“œ ì°¾ê¸°
        article_cards = soup.select('div.card_body')[:max_articles]
        
        for card in article_cards:
            try:
                # ì œëª©ê³¼ ë§í¬
                title_elem = card.select_one('h2.headline a')
                if not title_elem:
                    continue
                    
                title = title_elem.text.strip()
                link = title_elem.get('href', '#')
                
                # ë‚ ì§œ
                date_elem = card.select_one('p.date')
                date = date_elem.text.strip() if date_elem else datetime.now().strftime("%Y-%m-%d")
                
                # ìš”ì•½
                summary_elem = card.select_one('p.description')
                summary = summary_elem.text.strip() if summary_elem else title
                
                # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                category = "ì¼ë°˜"
                for cat, keywords in TRENDING_TOPICS.items():
                    if any(kw in title or kw in summary for kw in keywords):
                        category = cat
                        break
                
                articles.append({
                    "title": title,
                    "content": summary,
                    "date": date[:10] if len(date) > 10 else date,
                    "category": category,
                    "url": link,
                    "source": "ì¤‘ì•™ì¼ë³´"
                })
                
            except Exception as e:
                continue
        
        return articles
        
    except Exception as e:
        st.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

# ê²€ìƒ‰ í•¨ìˆ˜
def search_articles(query, articles):
    """ê¸°ì‚¬ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    if not articles:
        return []
    
    query_lower = query.lower()
    results = []
    
    for article in articles:
        score = 0
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in query_lower.split():
            if keyword in article["title"].lower():
                score += 5
            if keyword in article["content"].lower():
                score += 3
            if keyword in article.get("category", "").lower():
                score += 2
        
        if score > 0:
            results.append((score, article))
    
    results.sort(key=lambda x: x[0], reverse=True)
    return [r[1] for r in results[:5]]

# GPT ì‘ë‹µ ìƒì„±
def get_gpt_response(query, articles):
    if not api_key:
        return simple_response(query, articles)
    
    try:
        client = OpenAI(api_key=api_key.strip())
        
        context = "ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬:\n\n"
        for i, article in enumerate(articles[:5]):
            context += f"[ê¸°ì‚¬ {i+1}]\n"
            context += f"ì œëª©: {article['title']}\n"
            context += f"ë‚ ì§œ: {article['date']}\n"
            context += f"ì¹´í…Œê³ ë¦¬: {article.get('category', 'ì¼ë°˜')}\n"
            context += f"ë‚´ìš©: {article['content']}\n\n"
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  í†µì°°ë ¥ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."},
                {"role": "user", "content": f"{context}\nì§ˆë¬¸: {query}"}
            ],
            temperature=0.7,
            max_tokens=800
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        return simple_response(query, articles)

# ê°„ë‹¨í•œ ì‘ë‹µ
def simple_response(query, articles):
    if not articles:
        return "ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    response = f"ğŸ“° '{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼:\n\n"
    for i, article in enumerate(articles[:3]):
        response += f"**{i+1}. {article['title']}**\n"
        response += f"- ğŸ“… {article['date']} | ğŸ·ï¸ {article.get('category', 'ì¼ë°˜')}\n"
        response += f"- {article['content'][:150]}...\n\n"
    
    return response

# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "articles" not in st.session_state:
    st.session_state.articles = []
if "search_keyword" not in st.session_state:
    st.session_state.search_keyword = ""

# ë‰´ìŠ¤ ìˆ˜ì§‘ ì„¹ì…˜
st.sidebar.header("ğŸ” ë‰´ìŠ¤ ìˆ˜ì§‘")

# ê²€ìƒ‰ í‚¤ì›Œë“œ ì…ë ¥
search_keyword = st.sidebar.text_input(
    "ê²€ìƒ‰ í‚¤ì›Œë“œ", 
    placeholder="ì˜ˆ: AI, ê²½ì œ, ì„ ê±°, ë‚ ì”¨ ë“±",
    help="ê²€ìƒ‰í•˜ê³  ì‹¶ì€ ë‰´ìŠ¤ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”"
)

# ê¸°ì‚¬ ìˆ˜ ì„¤ì •
num_articles = st.sidebar.slider("ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜", 5, 20, 10)

# ë‰´ìŠ¤ ìˆ˜ì§‘ ë²„íŠ¼
col1, col2 = st.sidebar.columns(2)
with col1:
    if st.button("ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘", type="primary", use_container_width=True):
        if search_keyword:
            with st.spinner(f"'{search_keyword}' ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰ ì¤‘..."):
                articles = crawl_joongang_news(search_keyword, num_articles)
                if articles:
                    st.session_state.articles = articles
                    st.session_state.search_keyword = search_keyword
                    st.success(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ!")
                else:
                    st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.error("ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")

with col2:
    if st.button("ğŸ—‘ï¸ ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.messages = []
        st.session_state.articles = []
        st.rerun()

# ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì •ë³´
if st.session_state.articles:
    st.sidebar.success(f"ğŸ“Š '{st.session_state.search_keyword}' ê²€ìƒ‰ ê²°ê³¼: {len(st.session_state.articles)}ê°œ")
    
    with st.sidebar.expander("ğŸ“‹ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡"):
        for i, art in enumerate(st.session_state.articles[:10]):
            st.write(f"{i+1}. {art['title'][:30]}...")
            st.caption(f"{art['date']} | {art.get('category', 'ì¼ë°˜')}")

# ìµœì‹  ì´ìŠˆ ë° í‚¤ì›Œë“œ
st.sidebar.header("ğŸ”¥ ìµœì‹  ì´ìŠˆ")

# ì¹´í…Œê³ ë¦¬ë³„ íƒ­
tabs = st.sidebar.tabs(list(TRENDING_TOPICS.keys()))

for i, (category, keywords) in enumerate(TRENDING_TOPICS.items()):
    with tabs[i]:
        st.write(f"**{category} ê´€ë ¨ í‚¤ì›Œë“œ:**")
        # í‚¤ì›Œë“œ ë²„íŠ¼ë“¤
        for keyword in keywords:
            if st.button(keyword, key=f"trend_{category}_{keyword}", use_container_width=True):
                # ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì • í›„ ë‰´ìŠ¤ ìˆ˜ì§‘
                with st.spinner(f"'{keyword}' ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
                    articles = crawl_joongang_news(keyword, 10)
                    if articles:
                        st.session_state.articles = articles
                        st.session_state.search_keyword = keyword
                        st.rerun()

# ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
if not st.session_state.articles:
    # í™˜ì˜ ë©”ì‹œì§€
    st.info("""
    ğŸ¯ **ì‚¬ìš© ë°©ë²•:**
    1. ì‚¬ì´ë“œë°”ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê±°ë‚˜
    2. ìµœì‹  ì´ìŠˆì—ì„œ ê´€ì‹¬ ìˆëŠ” í‚¤ì›Œë“œë¥¼ í´ë¦­í•˜ì—¬
    3. ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•œ í›„ ì§ˆë¬¸í•´ë³´ì„¸ìš”!
    
    ğŸ’¡ **ê²€ìƒ‰ ê°€ëŠ¥í•œ ì£¼ì œ:** ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ, IT, ë¬¸í™”, ìŠ¤í¬ì¸  ë“± ëª¨ë“  ë¶„ì•¼
    """)
else:
    st.success(f"ğŸ“° '{st.session_state.search_keyword}' ê´€ë ¨ {len(st.session_state.articles)}ê°œ ê¸°ì‚¬ë¥¼ ë¶„ì„í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!")

# ëŒ€í™” í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”"):
    # ì‚¬ìš©ì ë©”ì‹œì§€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)
    
    # ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        if not st.session_state.articles:
            response = "ë¨¼ì € ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”! ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ìµœì‹  ì´ìŠˆì—ì„œ í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš”."
            st.write(response)
        else:
            with st.spinner("ë¶„ì„ ì¤‘..."):
                relevant = search_articles(prompt, st.session_state.articles)
                
                if relevant:
                    response = get_gpt_response(prompt, relevant)
                else:
                    response = f"'{prompt}'ì™€ ê´€ë ¨ëœ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. '{st.session_state.search_keyword}' ê´€ë ¨ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”."
                
                st.write(response)
                
                # ê´€ë ¨ ê¸°ì‚¬ ë§í¬
                if relevant:
                    with st.expander("ğŸ“ ì°¸ê³  ê¸°ì‚¬"):
                        for art in relevant[:3]:
                            if art['url'] != '#':
                                st.write(f"- [{art['title']}]({art['url']})")
                            else:
                                st.write(f"- {art['title']}")
                            st.caption(f"  {art['date']} | {art.get('category', 'ì¼ë°˜')}")
    
    st.session_state.messages.append({"role": "assistant", "content": response})

# í•˜ë‹¨ ì •ë³´
st.sidebar.markdown("---")
st.sidebar.info("""
**ğŸ’¡ íŒ:**
- êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•˜ë©´ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ì—¬ëŸ¬ í‚¤ì›Œë“œë¥¼ ì¡°í•©í•´ë³´ì„¸ìš” (ì˜ˆ: "AI ê·œì œ", "ê²½ì œ ì „ë§")
- ìˆ˜ì§‘ëœ ë‰´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µ ë¶„ì„ì„ ìš”ì²­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
""")
